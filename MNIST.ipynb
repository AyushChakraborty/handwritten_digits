{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing Convolutional Neural Net to learn handwritten digits, weights and biases written explicitely without using any prebuild layers from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset #type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ylecun/mnist\")   #the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ds['train']\n",
    "test_dataset = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = ds['train'][0]\n",
    "image = example['image']\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f\"label: {example['label']}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.tensor(np.array(image), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to implement CNN for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t6/s1wr8gp14gl28npcdk0thstc0000gn/T/ipykernel_1324/1360348788.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  X_tr = torch.tensor([np.array(img, dtype=np.float32)/255 for img in ds['train']['image']])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = torch.tensor([np.array(img, dtype=np.float32)/255 for img in ds['train']['image']])\n",
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te = torch.tensor([np.array(img, dtype=np.float32)/255 for img in ds['test']['image']])\n",
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr = torch.tensor(ds['train']['label'], dtype=torch.int32)\n",
    "Y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_te = torch.tensor(ds['test']['label'], dtype=torch.int32)\n",
    "Y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting X_tr to (60000, 196, 4) dim tensor\n",
    "X_tr_unfolded = X_tr.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(60000, -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 196, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_unfolded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((196, 4))\n",
    "b1 = torch.randn((196, ))\n",
    "\n",
    "l1 = (W1*X_tr_unfolded).sum(dim=2) + b1 #element wise mult done by * and summation along dim 2, 14*14\n",
    "\n",
    "W2 = torch.randn((196, 196))\n",
    "b2 = torch.randn((196, ))\n",
    "\n",
    "l2 = (l1@W2) + b2  #14*14\n",
    "#changing the view of l2 \n",
    "l2 = l2.view(60000, 14, 14)\n",
    "l2_unfolded = l2.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(60000, -1, 4)\n",
    "\n",
    "W3 = torch.randn((49, 4))\n",
    "b3 = torch.randn((49, ))\n",
    "\n",
    "l3 = (W3*l2_unfolded).sum(dim=2) + b3  #7*7\n",
    "\n",
    "W4 = torch.randn((49, 49))\n",
    "b4 = torch.randn((49, ))\n",
    "\n",
    "l4 = (l3@W4) + b4  # 7*7\n",
    "\n",
    "W5 = torch.randn((49, 10))\n",
    "b5 = torch.randn((10, ))\n",
    "\n",
    "logits = (l4@W5) + b5\n",
    "\n",
    "#loss\n",
    "nll = F.cross_entropy(logits, Y_tr.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1816.4763)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll  #loss of 1816 at the start, lets see how this can be reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8], dtype=torch.int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 196])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing it fresh from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((196, 4), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((196, ), generator=g, requires_grad=True)\n",
    "\n",
    "W2 = torch.randn((196, 196), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((196, ), generator=g, requires_grad=True)\n",
    "\n",
    "W3 = torch.randn((49, 4), generator=g, requires_grad=True)\n",
    "b3 = torch.randn((49, ), generator=g, requires_grad=True)\n",
    "\n",
    "W4 = torch.randn((49, 49), generator=g, requires_grad=True)\n",
    "b4 = torch.randn((49, ), generator=g, requires_grad=True)\n",
    "\n",
    "W5 = torch.randn((49, 10), generator=g, requires_grad=True)\n",
    "b5 = torch.randn((10, ), generator=g, requires_grad=True)\n",
    "\n",
    "parameters = [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42787"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)  #num of params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent \n",
    "for _ in range(3000):\n",
    "    #forward pass\n",
    "    l1 = (W1*X_tr_unfolded).sum(dim=2) + b1\n",
    "    \n",
    "    l2 = (l1@W2) + b2  \n",
    "    l2 = l2.view(60000, 14, 14)\n",
    "    l2_unfolded = l2.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(60000, -1, 4)\n",
    "\n",
    "    l3 = (W3*l2_unfolded).sum(dim=2) + b3\n",
    "\n",
    "    l4 = (l3@W4) + b4\n",
    "\n",
    "    logits = (l4@W5) + b5\n",
    "\n",
    "    nll = F.cross_entropy(logits, Y_tr.long())\n",
    "    print(f\"loss at epoch {_+1}: {nll.item()}\")\n",
    "\n",
    "\n",
    "    #backprop\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    nll.backward()\n",
    "\n",
    "\n",
    "    #update params\n",
    "    alpha = 0.001\n",
    "    for p in parameters:\n",
    "        # if (p.grad < 0):  #to stop training if we overshot local minima\n",
    "        #     break\n",
    "        # else:\n",
    "        p.data += -alpha*p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3497, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#egs code below\n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at test: 4.447265148162842\n"
     ]
    }
   ],
   "source": [
    "#test loss\n",
    "\n",
    "X_te_unfolded = X_te.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(10000, -1, 4)\n",
    "\n",
    "l1 = (W1*X_te_unfolded).sum(dim=2) + b1\n",
    "\n",
    "l2 = (l1@W2) + b2  \n",
    "l2 = l2.view(10000, 14, 14)\n",
    "l2_unfolded = l2.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(10000, -1, 4)\n",
    "\n",
    "l3 = (W3*l2_unfolded).sum(dim=2) + b3\n",
    "\n",
    "l4 = (l3@W4) + b4\n",
    "\n",
    "logits = (l4@W5) + b5\n",
    "\n",
    "nll = F.cross_entropy(logits, Y_te.long())\n",
    "print(f\"loss at test: {nll.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_logits(X):\n",
    "    X_unfolded = X.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(X.shape[0], -1, 4)\n",
    "\n",
    "    l1 = (W1*X_unfolded).sum(dim=2) + b1\n",
    "\n",
    "    l2 = (l1@W2) + b2  \n",
    "    l2 = l2.view(X.shape[0], 14, 14)\n",
    "    l2_unfolded = l2.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(X.shape[0], -1, 4)\n",
    "\n",
    "    l3 = (W3*l2_unfolded).sum(dim=2) + b3\n",
    "\n",
    "    l4 = (l3@W4) + b4\n",
    "\n",
    "    logits = (l4@W5) + b5\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n",
      "tensor([7, 2, 1,  ..., 4, 5, 6], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#getting some predictions\n",
    "logits_1 = out_logits(X_te)\n",
    "probs = F.softmax(logits_1, dim=1)\n",
    "# print(probs)\n",
    "\n",
    "max_probs, max_indices = torch.max(probs, dim=1)\n",
    "print(max_indices)\n",
    "print(Y_te)   #almost all match\n",
    "#loss went down from 1700 to 4, pretty good amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [[\n",
    "        [1, 2, 3, 4],\n",
    "        [11, 12, 13, 14],\n",
    "        [21, 22, 23, 24],\n",
    "        [31, 32, 33, 34]\n",
    "    ], \n",
    "    [\n",
    "        [100, 200, 300, 400],\n",
    "        [110, 120, 130, 140],\n",
    "        [210, 220, 230, 240],\n",
    "        [310, 320, 330, 340]\n",
    "    ]\n",
    "    ]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_un = X.unfold(2, 2, 2).unfold(3, 2, 2).contiguous().view(2, -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((4, 4))\n",
    "l = (W*X_un).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1.,   2.,  11.,  12.],\n",
       "         [  3.,   4.,  13.,  14.],\n",
       "         [ 21.,  22.,  31.,  32.],\n",
       "         [ 23.,  24.,  33.,  34.]],\n",
       "\n",
       "        [[100., 200., 110., 120.],\n",
       "         [300., 400., 130., 140.],\n",
       "         [210., 220., 310., 320.],\n",
       "         [230., 240., 330., 340.]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  24.7785,    1.9992,  -18.1983,  -24.0900],\n",
       "        [ 737.3009, -781.8102, -181.9827, -240.8997]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
